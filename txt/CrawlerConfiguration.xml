<?xml version="1.0" encoding="ISO-8859-1"?>

<!DOCTYPE configuration [
  <!ENTITY amp "&#x26;">
  <!ENTITY lt "&#x3C;">
]>

<configuration xmlns='http://xml.netbeans.org/examples/targetNS'
  xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
  xsi:schemaLocation='http://xml.netbeans.org/examples/targetNS file:SpiderConfiguration.xsd'>


<!-- Proxy settings -->
<proxy>
  <!--
  <host>proxy</host>
  <port>3128</port>
  <user>HansWurst</user>
  <password>gkxy23</password>
  -->
</proxy>


<!-- Specifies, whether to load URLs that are neither parsed nor indexed -->
<loadUnparsedUrls>false</loadUnparsedUrls>


<!--
 | Der Timeout für HTTP-Downloads. Dieser Wert bestimmt die maximale Zeit
 | in Sekunden, die ein HTTP-Download insgesamt dauern darf.
 +-->
<httpTimeout>180</httpTimeout>


<!--
 | The regular expressions that find the start and end locations of the content
 | that should be indexed in HTML pages.
 |
 | When the content should be indexed from the start, specify an empty text for
 | <startRegex>
 |
 | When the content should be indexed to the end, specify an empty text for
 | <endRegex>
 +-->
<htmlContentExtractorList>
  <contentExtractor>
    <prefix>http://www.testit.de/</prefix>
    <startRegex>&lt;!-- Start content --&gt;</startRegex>
    <!-- <endRegex></endRegex> -->
    <headlineRegex regexGroup="1">&lt;a name=.*>(.*)&lt;/a></headlineRegex>
  </contentExtractor>
</htmlContentExtractorList>


<!--
 | The ectractor that extracts the navigation path in HTML pages.
 |
 | <prefix>:        The prefix the path extractor is responsible for.
 | <startRegex>:    The regular expression that finds the start of the area
 |                  where the whole navigation path is.
 | <endRegex>:      The regular expression that finds the end of the area where
 |                  the whole navigation path is.
 | <pathNodeRegex>: The regular expression that extracts one node of the
 |                  navigation path.
 +-->
<htmlPathExtractorList>
  <pathExtractor>
    <prefix>http://www.testit.de/</prefix>
    <startRegex>&lt;!-- Pfad-Beginn  --&gt;</startRegex>
    <endRegex>&lt;!-- Pfad-Ende --&gt;</endRegex>
    <pathNodeRegex urlRegexGroup="1" titleRegexGroup="2">&lt;a.*href="([^"]*)">(.*)&lt;/a></pathNodeRegex>
  </pathExtractor>
</htmlPathExtractorList>


<!--
 | The list of patterns a document's URL must match to, when the link text
 | should be used as title instead of the document's real title.
 +-->
<useLinkTextAsTitleList>
  <urlPattern>^http://.*\.(pdf|xls|doc|rtf)$</urlPattern>
</useLinkTextAsTitleList>


<!-- The preferences for the search index -->
<searchIndex>
  <!-- The directory where the index should be located -->
  <dir>c:\Temp\searchindex</dir>

  <!-- Specifies, whether the index should be built -->
  <buildIndex>true</buildIndex>

  <!--
   | Specifies the analyzer type to use.
   |
   | The following types are supported:
   |  * english: For the english language
   |  * german: For the german language
   |
   | You can add more types by extending the method createAnalyser in the class
   | de.filiadata.lucene.LuceneToolkit.
   +-->
  <analyzerType>german</analyzerType>

  <!--
   | Specifies, whether the analysis files should be written.
   | The analysis files help to check the quality of the index building process.
   +-->
  <writeAnalysisFiles>true</writeAnalysisFiles>

  <!--
   | Gibt den maximalen Prozentsatz von gescheiterten Dokumenten an. (0..100)
   |
   | Ist das Verhältnis von gescheiterten Dokumenten zur Gesamtzahl von Dokumenten
   | größer als dieser Prozentsatz, so wird der Index verworfen.
   |
   | Gescheiterte Dokumente sind Dokumente die es entweder nicht gibt (Deadlink)
   | oder die nicht ausgelesen werden konnten.
   +-->
  <maxFailedDocuments>9.5</maxFailedDocuments>

  <!--
   | Contains all words that should not be indexed.
   | Separate the words by a blank.
   +-->
  <stopwordList>
    einer eine eines einem einen der die das dass daß du er sie es was wer wie
    wir und oder ohne mit am im in aus auf ist sein war wird ihr ihre ihres als
    für von mit dich dir mich mir mein sein kein durch wegen wird
  </stopwordList>

  <!--
   | Contains all words that should not be changed by an analyser when indexed.
   | Separate the words by a blank.
   +-->
  <exclusionList></exclusionList>

</searchIndex>


<controlFiles>
  <!--
   | Der Name der Kontrolldatei für erfolgreiche Indexerstellung.
   |
   | Diese Datei wird erzeugt, wenn der Index erstellt wurde, ohne dass
   | fatale Fehler aufgetreten sind.
   +-->
  <finishedWithoutFatalsFile>c:\Temp\control\NoFatals</finishedWithoutFatalsFile>

  <!--
   | Der Name der Kontrolldatei für fehlerhafte Indexerstellung.
   |
   | Diese Datei wird erzeugt, wenn der Index erstellt wurde, wobei
   | fatale Fehler aufgetreten sind.
   +-->
  <finishedWithFatalsFile>c:\Temp\control\WithFatals</finishedWithFatalsFile>
</controlFiles>


<preparatorList>
  <preparator>
    <urlPattern>\.(/|html|htm)$</urlPattern>
    <class>.HtmlPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.txt$</urlPattern>
    <class>.PlainTextPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.xml$</urlPattern>
    <class>.XmlPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.pdf$</urlPattern>
    <class>.PdfPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.xls$</urlPattern>
    <!--<class>.PoiMsExcelPreparator</class>-->
    <class>.JacobMsExcelPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.doc$</urlPattern>
    <!--<class>.PoiMsWordPreparator</class>-->
    <class>.JacobMsWordPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.ppt$</urlPattern>
    <class>.JacobMsPowerPointPreparator</class>
  </preparator>
  <preparator>
    <urlPattern>\.rtf$</urlPattern>
    <!--<class>.SwingRtfPreparator</class>-->
    <class>.SimpleRtfPreparator</class>
  </preparator>
</preparatorList>


<!-- The regular expressions that indetify URLs in HTML. -->
<htmlParserPatternList>
  <pattern parse="true" index="true" regexGroup="1">="([^"]*\.html)"</pattern>
  <pattern parse="false" index="true" regexGroup="1">="([^"]*\.(pdf|xls|doc|rtf))"</pattern>
  <pattern parse="false" index="false" regexGroup="1">="([^"]*\.(js|css|jpg|gif|png))"</pattern>
</htmlParserPatternList>


<!--
 | The regular expressions that identify file names that should be processed
 | by the spider.
 +-->
<directoryParserPatternList>
  <pattern index="true">^.*\.(html|pdf|ppt|xls|doc|rtf)$</pattern>
</directoryParserPatternList>


<!-- The list of URLs where the spidering will start. -->
<startlist>
  <!-- Directory parsing -->
  <start parse="true" index="false">file://c:/Temp/dokumente</start>

  <!-- HTML parsing -->
  <!--
  <start parse="true" index="true">http://www.mydomain.de/some/path/</start>
  -->
</startlist>


<!-- The blacklist containing prefixes an URL must NOT have to be processed -->
<blacklist>
  <prefix>http://www.mydomain.de/some/dynamic/content/</prefix>
</blacklist>


<!-- The whitelist containing prefixes an URL must have to be processed -->
<whitelist>
  <prefix name="file">file://</prefix>
  <prefix>http://www.mydomain.de</prefix>
</whitelist>

</configuration>
